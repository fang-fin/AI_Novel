# ============================================
# LLM API Configuration
# ============================================

# Provider: openai, anthropic, grok, deepseek, custom (compatible with OpenAI API)
LLM_PROVIDER=deepseek

# ============================================
# OpenAI Configuration
# ============================================
# For OpenAI API (api.openai.com)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o

# ============================================
# Anthropic Configuration
# ============================================
# For Anthropic API (api.anthropic.com)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ============================================
# Grok (xAI) Configuration
# ============================================
# For Grok API (api.x.ai)
GROK_API_KEY=your_grok_api_key_here
GROK_MODEL=grok-4-fast-non-reasoning

# ============================================
# DeepSeek Configuration
# ============================================
# For DeepSeek API (api.deepseek.com)
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat

# ============================================
# Custom / Self-hosted LLM Configuration
# ============================================
# For vLLM, Ollama, LM Studio, or other OpenAI-compatible APIs
CUSTOM_API_BASE=http://localhost:8000/v1
CUSTOM_API_KEY=your_api_key_here
CUSTOM_MODEL=your_model_name

# ============================================
# Generation Settings (Optional)
# ============================================
# DEFAULT_MAX_TOKENS=8192
# DEFAULT_TEMPERATURE=0.75
# DEFAULT_TIMEOUT=600

# ============================================
# Proxy Configuration (Optional)
# ============================================
# HTTP_PROXY=http://172.30.96.1:7890
# HTTPS_PROXY=http://172.30.96.1:7890
